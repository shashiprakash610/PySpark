# ðŸ‡©ðŸ‡ª German Unemployment Analytics Pipeline

**A PySpark-based ETL pipeline for processing official unemployment statistics from the Federal Employment Agency (BA Statistik)**

[![Spark](https://img.shields.io/badge/Apache_Spark-3.5.5-E25A1C)](https://spark.apache.org/)
[![Python](https://img.shields.io/badge/Python-3.12%2B-3776AB)](https://python.org)
[![License](https://img.shields.io/badge/license-BSD--3--Clause-blue)](LICENSE)

---

## ðŸ“Œ Key Features

- ðŸ”„ **ETL Pipeline with PySpark**
  - Reads raw `.csv` files from BA Statistik (2005â€“2025)
  - Skips malformed rows, standardizes structure, adds metadata
- ðŸ§± **Schema-Driven Design**
  - Explicit Spark schemas for each dataset category
- ðŸ“Š **Time-Series Ready**
  - Parses dates, extracts `year` and `month`, and aggregates over time
- ðŸ“¦ **Outputs to Parquet**
  - Clean data is saved in optimized columnar format
- ðŸŒ **Visualizations Added**
  - Line plots showing trends across years and categories

---

## ðŸ“‚ Dataset Categories

| Category             | Examples                                | Scope                  |
|----------------------|-----------------------------------------|------------------------|
| Unemployment Count   | `arbeitslose_deutschland_originalwert.csv` | Germany, East, West    |
| Unemployment Rate    | `arbeitslosenquote_*.csv`               | Germany, East, West    |
| Seasonally Adjusted  | `kurzarbeiter_bv41.csv`, `stellen.csv` | National Level         |

---

## ðŸ› ï¸ Tech Stack

| Layer        | Tool           |
|--------------|----------------|
| Processing   | PySpark        |
| Storage      | Parquet Files  |
| Scripting    | Python 3.12    |
| Visualization| Matplotlib, Seaborn |
| IDE Tested On| VSCode + MacOS |

---

## ðŸš€ Getting Started

### ðŸ”§ Prerequisites

```bash
# Install Python & Java
brew install python openjdk@11

# Set up virtual environment
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### ðŸ“‚ Project Structure
```
SPARK-ETL-PROJECT/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ schemas.py             # Spark schemas
â”œâ”€â”€ Data/
â”‚   â”œâ”€â”€ Raw/                   # Input CSVs
â”‚   â””â”€â”€ processed/             # Output Parquet files
â”œâ”€â”€ outputs/                  # Saved visualization images
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ load.py                # ETL driver
â”‚   â”œâ”€â”€ transform.py           # Column transformation & metadata
â”‚   â”œâ”€â”€ analyze.py             # Time-based aggregations
â”‚   â””â”€â”€ visualize.py           # Matplotlib/Seaborn visualizations
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ðŸ“… How to Run the Pipeline

```bash
# 1. Activate virtual environment (if not already)
source venv/bin/activate

# 2. Run the ETL process
python scripts/load.py

# 3. Run data analysis
python scripts/analyze.py

# 4. Generate graphs
python scripts/visualize.py
```

---

## ðŸ” Sample Visual Output
> Output graphs are saved in the `outputs/` directory.

### ðŸ“ˆ Unemployment Over Time
![Unemployment Trend](outputs/unemployment_trend.png)

### ðŸ§‘â€ðŸ¤â€ðŸ§‘ Gender-wise Unemployment Trend
![Gender Trend](outputs/unemployment_gender_trend.png)

### ðŸ‘¶ Youth vs Long-term Unemployment
![Youth vs Long Term](outputs/unemployment_youth_vs_longterm.png)


---

## ðŸ“Š Showcase This Project 

> "Developed a PySpark-based ETL pipeline for time-series analysis of German unemployment data. Integrated multi-source datasets, enforced schema-driven validation, and automated visual reporting using Matplotlib."

---

## âœ… License